{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georopeza/Georopeza-Optimizacion-Rutas-QLearning/blob/main/IAAA_P3_FernandezMariangel_OropezaGerman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports, Ciudades y distancias"
      ],
      "metadata": {
        "id": "aW_sSe31L36H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importaciones\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import pandas as pd # Importa pandas para una mejor visualización de la matriz\n",
        "import matplotlib.pyplot as plt # Importa matplotlib para graficar\n",
        "\n",
        "# Permite al usuario ingresar el número de ciudades\n",
        "while True:\n",
        "    num_ciudades = int(input(\"Ingrese el número de ciudades (mínimo 3, máximo 10): \"))\n",
        "    if 3 <= num_ciudades <= 15: #de 3 a 10\n",
        "        break\n",
        "    else:\n",
        "        print(\"Número de ciudades inválido. Por favor, ingrese un número entre 3 y 10.\")\n",
        "\n",
        "# Permite al usuario ingresar las coordenadas o generarlas al azar\n",
        "coords_input = input(\"¿Desea ingresar las coordenadas manualmente (m) o generarlas al azar (a)? \")\n",
        "if coords_input.lower() == 'm':\n",
        "    coordenadas = []\n",
        "    for i in range(num_ciudades):\n",
        "        x = float(input(f\"Ingrese la coordenada x para la ciudad {i}: \"))\n",
        "        y = float(input(f\"Ingrese la coordenada y para la ciudad {i}: \"))\n",
        "        coordenadas.append((x, y))\n",
        "else:\n",
        "    # Genera coordenadas al azar como enteros entre 0 y 100\n",
        "    coordenadas = [(random.randint(0, 100), random.randint(0, 100)) for _ in range(num_ciudades)]\n",
        "\n",
        "# Muestra las coordenadas\n",
        "print(\"\\nCoordenadas de las ciudades:\")\n",
        "for i, (x, y) in enumerate(coordenadas):\n",
        "    print(f\"Ciudad {i}: ({x:.2f}, {y:.2f})\")\n",
        "\n",
        "\n",
        "# Implementa la función para calcular la distancia euclidiana\n",
        "def distancia_euclidiana(ciudad1, ciudad2):\n",
        "    return math.sqrt((ciudad1[0] - ciudad2[0])**2 + (ciudad1[1] - ciudad2[1])**2)\n",
        "\n",
        "# Crea una matriz de distancias\n",
        "matriz_distancias = np.zeros((num_ciudades, num_ciudades))\n",
        "for i in range(num_ciudades):\n",
        "    for j in range(num_ciudades):\n",
        "        matriz_distancias[i, j] = distancia_euclidiana(coordenadas[i], coordenadas[j]) #llama a la funcion distancia eucladiana para rellenar la matriz\n",
        "\n",
        "# Muestra la matriz de distancias usando un DataFrame de pandas\n",
        "print(\"\\nMatriz de distancias:\")\n",
        "distancia_df = pd.DataFrame(matriz_distancias, index=[f'Ciudad {i}' for i in range(num_ciudades)], columns=[f'Ciudad {j}' for j in range(num_ciudades)])\n",
        "display(distancia_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "3W6ga5m7L2fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algoritmo"
      ],
      "metadata": {
        "id": "gFeMaVJLL24I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parámetros de Q-Learning ---\n",
        "alpha = 0.4  # Tasa de aprendizaje\n",
        "gamma = 0.9  # Factor de descuento\n",
        "epsilon = 0.1  # Tasa de exploración\n",
        "num_episodios = 1000 # Número de episodios de entrenamiento\n",
        "epsilon_min = 0.05\n",
        "decay_rate = 0.995\n",
        "\n",
        "# Inicializa la tabla Q\n",
        "# El estado se representa como (ciudad_actual, bitmask_ciudades_visitadas)\n",
        "# El bitmask indica qué ciudades han sido visitadas (1 si visitada, 0 si no)\n",
        "# El número de estados es num_ciudades * 2^num_ciudades\n",
        "# Las acciones son las ciudades a las que se puede mover (0 a num_ciudades-1)\n",
        "q_table = np.zeros((num_ciudades, 2**num_ciudades, num_ciudades))\n",
        "\n",
        "# --- Funciones Auxiliares ---\n",
        "\n",
        "# Función para obtener el índice del estado a partir de la ciudad actual y el bitmask\n",
        "def get_estado_index(ciudad_actual, bitmask_visitadas):\n",
        "    return (ciudad_actual, bitmask_visitadas)\n",
        "\n",
        "# Función para obtener las acciones posibles (ciudades no visitadas)\n",
        "def get_acciones_posibles(bitmask_visitadas):\n",
        "    acciones_posibles = []\n",
        "    for ciudad in range(num_ciudades):\n",
        "        # Si el bit correspondiente es 0, la ciudad no ha sido visitada\n",
        "        if (bitmask_visitadas >> ciudad) & 1 == 0:\n",
        "            acciones_posibles.append(ciudad)\n",
        "    return acciones_posibles\n",
        "\n",
        "# Función para actualizar el bitmask de ciudades visitadas\n",
        "def actualizar_bitmask(bitmask_visitadas, ciudad_visitada):\n",
        "    return bitmask_visitadas | (1 << ciudad_visitada)\n",
        "\n",
        "# --- Algoritmo de Q-Learning ---\n",
        "\n",
        "print(\"\\nEntrenando el agente Q-Learning...\")\n",
        "\n",
        "# Lista para almacenar la distancia del camino encontrado al final de cada episodio para graficar la convergencia\n",
        "episode_distances = []\n",
        "large_positive_reward = 500.0 # Define una gran recompensa positiva por completar un tour válido\n",
        "\n",
        "coords_input = input(\"¿Desea ingresar la ciudad inicial manualmente (m) o generarla al azar (a)?\")\n",
        "if coords_input.lower() == 'm':\n",
        "        ciudad_inicio = int(input(f\"Ingrese la ciudad con la que desea iniciar: \"))\n",
        "else:\n",
        "    ciudad_inicio = random.randint(0, num_ciudades - 1)\n",
        "for episodio in range(num_episodios):\n",
        "    # El agente comienza en una ciudad aleatoria en cada episodio\n",
        "    ciudad_actual = ciudad_inicio\n",
        "    # Inicializa el bitmask indicando que solo la ciudad inicial ha sido visitada\n",
        "    bitmask_visitadas = 1 << ciudad_actual\n",
        "\n",
        "    recompensa_total_episodio = 0\n",
        "    pasos = 0\n",
        "\n",
        "    # El episodio termina cuando todas las ciudades han sido visitadas O si no hay acciones válidas posibles\n",
        "    while bitmask_visitadas < (2**num_ciudades) - 1:\n",
        "\n",
        "        estado_index = get_estado_index(ciudad_actual, bitmask_visitadas)\n",
        "        acciones_posibles = get_acciones_posibles(bitmask_visitadas)\n",
        "\n",
        "        # Si no hay acciones posibles (todas las ciudades visitadas), romper el bucle\n",
        "        if not acciones_posibles:\n",
        "            break\n",
        "\n",
        "        # Estrategia Epsilon-Greedy, para elegir la accion\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Exploración: elige una acción aleatoria entre las posibles\n",
        "            proxima_ciudad = random.choice(acciones_posibles)\n",
        "        else:\n",
        "            # Explotación: elige la acción con el mayor valor Q entre las posibles\n",
        "            q_valores_actuales = q_table[estado_index]\n",
        "            # Filtra los valores Q para solo las acciones posibles\n",
        "            q_valores_posibles = [q_valores_actuales[accion] for accion in acciones_posibles]\n",
        "            max_q = max(q_valores_posibles)\n",
        "            # Maneja el caso de múltiples acciones con el mismo valor Q máximo\n",
        "            mejores_acciones = [acciones_posibles[i] for i, q_val in enumerate(q_valores_posibles) if q_val == max_q]\n",
        "            proxima_ciudad = random.choice(mejores_acciones)\n",
        "\n",
        "        # Realiza la acción\n",
        "        # La recompensa inmediata es la distancia negativa\n",
        "        recompensa = -matriz_distancias[ciudad_actual, proxima_ciudad]\n",
        "        recompensa_total_episodio += recompensa\n",
        "\n",
        "        # Calcula el próximo estado\n",
        "        proximo_bitmask_visitadas = actualizar_bitmask(bitmask_visitadas, proxima_ciudad)\n",
        "        proximo_estado_index = get_estado_index(proxima_ciudad, proximo_bitmask_visitadas)\n",
        "\n",
        "        # Actualiza la tabla Q\n",
        "        # Encuentra el máximo valor Q para el próximo estado (considerando solo acciones posibles desde el próximo estado)\n",
        "        acciones_posibles_proximo = get_acciones_posibles(proximo_bitmask_visitadas)\n",
        "        if acciones_posibles_proximo:\n",
        "            max_q_s_prime_a_prime = np.max([q_table[proximo_estado_index][accion] for accion in acciones_posibles_proximo])\n",
        "        else:\n",
        "            # Si todas las ciudades han sido visitadas, el próximo estado es el estado final (regreso al inicio)\n",
        "            # La recompensa por este último paso se maneja fuera de este bucle while\n",
        "            max_q_s_prime_a_prime = 0 # No hay recompensas futuras desde un estado terminal\n",
        "\n",
        "\n",
        "        # Actualiza los valores de Q (Ecuación de Bellman)\n",
        "        q_table[estado_index][proxima_ciudad] = q_table[estado_index][proxima_ciudad] + alpha * (recompensa + gamma * max_q_s_prime_a_prime - q_table[estado_index][proxima_ciudad])\n",
        "\n",
        "        # Actualiza el estado actual\n",
        "        ciudad_actual = proxima_ciudad\n",
        "        bitmask_visitadas = proximo_bitmask_visitadas\n",
        "        pasos += 1 #contador de cuantas acciones ha tomado\n",
        "\n",
        "    # Después de visitar todas las ciudades, el agente regresa a la ciudad de inicio para cerrar el ciclo\n",
        "    # Verifica si todas las ciudades fueron visitadas (bitmask completo) y si el agente no está ya en el inicio\n",
        "    if bitmask_visitadas == (2**num_ciudades) - 1 and ciudad_actual != ciudad_inicio:\n",
        "        estado_index = get_estado_index(ciudad_actual, bitmask_visitadas)\n",
        "        proxima_ciudad = ciudad_inicio # La única acción posible es regresar al inicio\n",
        "\n",
        "        # Calcula la recompensa por regresar al inicio\n",
        "        # Esta recompensa incluye la distancia negativa Y la gran recompensa positiva por completar el tour\n",
        "        recompensa_retorno = -matriz_distancias[ciudad_actual, proxima_ciudad] + large_positive_reward\n",
        "        recompensa_total_episodio += recompensa_retorno # Agrega la recompensa neta al total\n",
        "\n",
        "        # Decaimiento de epsilon para que el agente explore menos y explote lo aprendido\n",
        "        epsilon = max(epsilon * decay_rate, epsilon_min)\n",
        "\n",
        "        # No hay un próximo estado después de regresar al inicio en un episodio\n",
        "        max_q_s_prime_a_prime = 0\n",
        "\n",
        "        # Actualiza la tabla Q para el último paso (regreso al inicio)\n",
        "        q_table[estado_index][proxima_ciudad] = q_table[estado_index][proxima_ciudad] + alpha * (recompensa_retorno + gamma * max_q_s_prime_a_prime - q_table[estado_index][proxima_ciudad])\n",
        "\n",
        "        ciudad_actual = proxima_ciudad # El agente ha regresado al inicio\n",
        "        pasos += 1\n",
        "    # Si el bucle terminó pero no todas las ciudades fueron visitadas (no debería ocurrir con la lógica actual, pero como medida de seguridad)\n",
        "    # o si el agente terminó en el inicio sin visitar todas las ciudades (tour inválido)\n",
        "    # podríamos agregar una penalización aquí si fuera necesario, pero la estructura de recompensa actual penaliza implícitamente los tours incompletos\n",
        "    # al no proporcionar la gran recompensa positiva.\n",
        "\n",
        "    # --- Evaluación al final de cada episodio para seguir la convergencia ---\n",
        "    # Encuentra el mejor camino usando la tabla Q actual (explotación pura)\n",
        "    # Comienza desde la ciudad inicial de este episodio para la evaluación\n",
        "    ciudad_inicio_eval = ciudad_inicio\n",
        "    camino_encontrado_eval = [ciudad_inicio_eval]\n",
        "    ciudad_actual_eval = ciudad_inicio_eval\n",
        "    bitmask_visitadas_eval = 1 << ciudad_inicio_eval\n",
        "    distancia_total_eval = 0\n",
        "\n",
        "    while bitmask_visitadas_eval < (2**num_ciudades) - 1:\n",
        "        estado_index_eval = get_estado_index(ciudad_actual_eval, bitmask_visitadas_eval)\n",
        "        acciones_posibles_eval = get_acciones_posibles(bitmask_visitadas_eval)\n",
        "\n",
        "        if not acciones_posibles_eval:\n",
        "             # Si no hay acciones posibles, romper - esto podría indicar un camino incompleto\n",
        "             distancia_total_eval = float('inf') # Asigna una distancia muy grande para caminos incompletos\n",
        "             break\n",
        "\n",
        "        q_valores_actuales_eval = q_table[estado_index_eval]\n",
        "        q_valores_posibles_eval = [q_valores_actuales_eval[accion] for accion in acciones_posibles_eval]\n",
        "        # Maneja el caso en el que todos los valores Q posibles son cero o negativos (estado inicial)\n",
        "        if not q_valores_posibles_eval:\n",
        "             # Si no hay acciones posibles en absoluto, algo está mal, romper\n",
        "             distancia_total_eval = float('inf')\n",
        "             break\n",
        "        else:\n",
        "            max_q_eval = max(q_valores_posibles_eval)\n",
        "            # Si todos los valores Q son negativos o cero, elige una acción aleatoria entre las posibles para intentar escapar\n",
        "            if max_q_eval <= 0:\n",
        "                 proxima_ciudad_eval = random.choice(acciones_posibles_eval)\n",
        "            else:\n",
        "                 # Elige la acción con el mayor valor Q positivo\n",
        "                 mejores_acciones_eval = [acciones_posibles_eval[i] for i, q_val in enumerate(q_valores_posibles_eval) if q_val == max_q_eval]\n",
        "                 proxima_ciudad_eval = random.choice(mejores_acciones_eval)\n",
        "\n",
        "\n",
        "        distancia_total_eval += matriz_distancias[ciudad_actual_eval, proxima_ciudad_eval]\n",
        "        camino_encontrado_eval.append(proxima_ciudad_eval)\n",
        "\n",
        "        ciudad_actual_eval = proxima_ciudad_eval\n",
        "        bitmask_visitadas_eval = actualizar_bitmask(bitmask_visitadas_eval, proxima_ciudad_eval)\n",
        "\n",
        "    # Agrega la distancia para regresar al inicio si se completó un tour completo\n",
        "    if bitmask_visitadas_eval == (2**num_ciudades) - 1 and ciudad_actual_eval != ciudad_inicio_eval:\n",
        "        distancia_total_eval += matriz_distancias[ciudad_actual_eval, ciudad_inicio_eval]\n",
        "        camino_encontrado_eval.append(ciudad_inicio_eval)\n",
        "    elif bitmask_visitadas_eval < (2**num_ciudades) - 1:\n",
        "        # Si el camino fue incompleto, establece la distancia a infinito o un valor grande\n",
        "        distancia_total_eval = float('inf')\n",
        "\n",
        "    # Almacena la distancia para este episodio\n",
        "    episode_distances.append(distancia_total_eval)\n",
        "\n",
        "\n",
        "    # Opcional: Imprimir la recompensa total del episodio para seguir el progreso\n",
        "    if (episodio + 1) % 100 == 0:\n",
        "        print(f\"Episodio {episodio + 1}/{num_episodios}, Recompensa total: {recompensa_total_episodio:.2f}, Distancia Evaluada: {distancia_total_eval:.2f}\")\n",
        "\n",
        "print(\"Entrenamiento completado.\")\n",
        "\n",
        "# --- Graficando la convergencia ---\n",
        "plt.figure(figsize=(30, 10))\n",
        "plt.plot(range(1, num_episodios + 1), episode_distances)\n",
        "plt.xlabel(\"Episodio\")\n",
        "plt.ylabel(\"Distancia Total del Camino Encontrado (Evaluación)\")\n",
        "plt.title(\"Convergencia del Agente Q-Learning\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Encuentra el mejor camino después del entrenamiento (usando la tabla Q final) ---\n",
        "\n",
        "print(\"\\nBuscando el mejor camino encontrado por el agente después del entrenamiento:\")\n",
        "\n",
        "# Comienza la evaluación desde la ciudad que fue el inicio del último episodio de entrenamiento\n",
        "ciudad_inicio_final_eval = ciudad_inicio\n",
        "camino_encontrado_final = [ciudad_inicio_final_eval]\n",
        "ciudad_actual_final_eval = ciudad_inicio_final_eval\n",
        "bitmask_visitadas_final_eval = 1 << ciudad_inicio_final_eval\n",
        "distancia_total_final_eval = 0\n",
        "\n",
        "while bitmask_visitadas_final_eval < (2**num_ciudades) - 1:\n",
        "    estado_index_final_eval = get_estado_index(ciudad_actual_final_eval, bitmask_visitadas_final_eval)\n",
        "    acciones_posibles_final_eval = get_acciones_posibles(bitmask_visitadas_final_eval)\n",
        "\n",
        "    if not acciones_posibles_final_eval:\n",
        "        print(\"Advertencia: El camino de evaluación no visitó todas las ciudades.\")\n",
        "        distancia_total_final_eval = float('inf')\n",
        "        break\n",
        "\n",
        "    q_valores_actuales_final_eval = q_table[estado_index_final_eval]\n",
        "    q_valores_posibles_final_eval = [q_valores_actuales_final_eval[accion] for accion in acciones_posibles_final_eval]\n",
        "\n",
        "    if not q_valores_posibles_final_eval:\n",
        "        distancia_total_final_eval = float('inf')\n",
        "        break\n",
        "    else:\n",
        "        max_q_final_eval = max(q_valores_posibles_final_eval)\n",
        "        # En la evaluación final, siempre elige la acción con el valor Q más alto (incluso si es negativo, para seguir la política aprendida)\n",
        "        mejores_acciones_final_eval = [acciones_posibles_final_eval[i] for i, q_val in enumerate(q_valores_posibles_final_eval) if q_val == max_q_final_eval]\n",
        "        # Elige la primera mejor acción para una evaluación determinista\n",
        "        proxima_ciudad_final_eval = mejores_acciones_final_eval[0]\n",
        "\n",
        "\n",
        "    distancia_total_final_eval += matriz_distancias[ciudad_actual_final_eval, proxima_ciudad_final_eval]\n",
        "    camino_encontrado_final.append(proxima_ciudad_final_eval)\n",
        "\n",
        "    ciudad_actual_final_eval = proxima_ciudad_final_eval\n",
        "    bitmask_visitadas_final_eval = actualizar_bitmask(bitmask_visitadas_final_eval, proxima_ciudad_final_eval)\n",
        "\n",
        "# Agrega la distancia para regresar al inicio si se completó un tour completo\n",
        "if bitmask_visitadas_final_eval == (2**num_ciudades) - 1 and ciudad_actual_final_eval != ciudad_inicio_final_eval:\n",
        "    distancia_total_final_eval += matriz_distancias[ciudad_actual_final_eval, ciudad_inicio_final_eval]\n",
        "    camino_encontrado_final.append(ciudad_inicio_final_eval)\n",
        "elif bitmask_visitadas_final_eval < (2**num_ciudades) - 1:\n",
        "    # Si el camino fue incompleto\n",
        "     print(\"Advertencia: El camino de evaluación final no visitó todas las ciudades.\")\n",
        "     distancia_total_final_eval = float('inf')\n",
        "\n",
        "\n",
        "print(\"Camino encontrado:\", \" -> \".join(map(str, camino_encontrado_final)))\n",
        "print(f\"Distancia total del camino encontrado: {distancia_total_final_eval:.2f}\")\n",
        "\n",
        "\n",
        "# --- Graficando el camino final con distancias ---\n",
        "if coords_input.lower() == 'a' or coords_input.lower() == 'm':\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    # Graficar las ciudades\n",
        "    x_coords = [c[0] for c in coordenadas]\n",
        "    y_coords = [c[1] for c in coordenadas]\n",
        "    plt.scatter(x_coords, y_coords, marker='o', s=100, color='red')\n",
        "    # Etiquetar las ciudades\n",
        "    for i in range(num_ciudades):\n",
        "        plt.text(x_coords[i], y_coords[i], str(i), fontsize=12, ha='right')\n",
        "\n",
        "    # Graficar el camino encontrado y añadir distancias\n",
        "    if len(camino_encontrado_final) > 1 and distancia_total_final_eval != float('inf'):\n",
        "        camino_x_final = [coordenadas[ciudad][0] for ciudad in camino_encontrado_final]\n",
        "        camino_y_final = [coordenadas[ciudad][1] for ciudad in camino_encontrado_final]\n",
        "        plt.plot(camino_x_final, camino_y_final, linestyle='-', color='blue', marker='o')\n",
        "\n",
        "        # Añade etiquetas de distancia al gráfico para los segmentos DENTRO del tour\n",
        "        for i in range(len(camino_encontrado_final) - 1):\n",
        "            ciudad1_index = camino_encontrado_final[i]\n",
        "            ciudad2_index = camino_encontrado_final[i+1]\n",
        "            if ciudad1_index != ciudad2_index: # Evita etiquetar la distancia de una ciudad a sí misma\n",
        "                 distance = matriz_distancias[ciudad1_index, ciudad2_index]\n",
        "                 mid_x = (coordenadas[ciudad1_index][0] + coordenadas[ciudad2_index][0]) / 2\n",
        "                 mid_y = (coordenadas[ciudad1_index][1] + coordenadas[ciudad2_index][1]) / 2\n",
        "                 plt.text(mid_x, mid_y, f\"{distance:.2f}\", fontsize=9, color='black', ha='center', va='bottom')\n",
        "\n",
        "\n",
        "        # Grafica la línea de regreso a la ciudad de inicio para cerrar el ciclo\n",
        "        if len(camino_encontrado_final) > num_ciudades and camino_encontrado_final[0] == camino_encontrado_final[-1]:\n",
        "             ciudad_final_index = camino_encontrado_final[-2]\n",
        "             ciudad_inicio_index = camino_encontrado_final[0]\n",
        "             if ciudad_final_index != ciudad_inicio_index:\n",
        "                 distance_return = matriz_distancias[ciudad_final_index, ciudad_inicio_index]\n",
        "                 mid_x_return = (coordenadas[ciudad_final_index][0] + coordenadas[ciudad_inicio_index][0]) / 2\n",
        "                 mid_y_return = (coordenadas[ciudad_final_index][1] + coordenadas[ciudad_inicio_index][1]) / 2\n",
        "                 plt.plot([coordenadas[ciudad_final_index][0], coordenadas[ciudad_inicio_index][0]],\n",
        "                          [coordenadas[ciudad_final_index][1], coordenadas[ciudad_inicio_index][1]],\n",
        "                          linestyle='--', color='gray')\n",
        "                 plt.text(mid_x_return, mid_y_return, f\"{distance_return:.2f}\", fontsize=9, color='black', ha='center', va='bottom')\n",
        "\n",
        "\n",
        "    plt.title(\"Camino final encontrado por el Agente Q-Learning con Distancias\")\n",
        "    plt.xlabel(\"Coordenada X\")\n",
        "    plt.ylabel(\"Coordenada Y\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UgRHp1Qp8rIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}